{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.multioutput import RegressorChain\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_steps(\n",
    "    df : pd.DataFrame, \n",
    "    y : str, \n",
    "    steps : int\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get target feature steps ahead.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas data frame\n",
    "        Data frame with target feature.\n",
    "    \n",
    "    y : str\n",
    "        Target feature name.\n",
    "    \n",
    "    steps : int\n",
    "        Steps to forecast.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas data frame\n",
    "        Data frame with target features.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(steps):\n",
    "        df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# models\n",
    "rid = RegressorChain(\n",
    "    base_estimator=RidgeCV(\n",
    "        alphas=[1e-3, 1e-2, 1e-1, 1],\n",
    "        cv=TimeSeriesSplit(n_splits=3, test_size=2000),\n",
    "    ),\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "gb = RegressorChain(\n",
    "    base_estimator=HalvingGridSearchCV(\n",
    "        estimator=HistGradientBoostingRegressor(random_state=123),\n",
    "        param_grid = {\n",
    "            \"max_depth\": np.arange(10, 40, 10, dtype=int),\n",
    "            \"learning_rate\": np.logspace(-3, -1, 3)\n",
    "        },\n",
    "        cv=TimeSeriesSplit(n_splits=3, test_size=2000),\n",
    "        aggressive_elimination=True,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        random_state=123,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "mlp = RegressorChain(\n",
    "    base_estimator=HalvingGridSearchCV(\n",
    "        estimator=MLPRegressor(max_iter=500, random_state=123),\n",
    "        param_grid = {\n",
    "            \"hidden_layer_sizes\": [(100,), (100, 75, 25)],\n",
    "            \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"]\n",
    "        },\n",
    "        cv=TimeSeriesSplit(n_splits=3, test_size=2000),\n",
    "        aggressive_elimination=True,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        random_state=123,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"rid\": rid,\n",
    "    \"mlp\": mlp,\n",
    "    \"gb\": gb,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials to read files on S3 bucket\n",
    "f = open('../credentials.json')\n",
    "credentials = json.load(f)\n",
    "\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=credentials[\"Access key ID\"],\n",
    "    aws_secret_access_key=credentials[\"Secret access key\"]\n",
    "    )\n",
    "\n",
    "s3_resource = boto3.resource(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=credentials[\"Access key ID\"],\n",
    "    aws_secret_access_key=credentials[\"Secret access key\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processed/',\n",
       " 'processed/anhembi.csv',\n",
       " 'processed/butanta.csv',\n",
       " 'processed/campo_limpo.csv',\n",
       " 'processed/capela_do_socorro.csv',\n",
       " 'processed/cge_clusters.csv',\n",
       " 'processed/cidade_ademar.csv',\n",
       " 'processed/cluster_A.csv',\n",
       " 'processed/cluster_B.csv',\n",
       " 'processed/cluster_C.csv',\n",
       " 'processed/cluster_D.csv',\n",
       " 'processed/freguesia_do_o.csv',\n",
       " 'processed/ipiranga.csv',\n",
       " 'processed/itaim_paulista.csv',\n",
       " 'processed/itaquera.csv',\n",
       " 'processed/jabaquara.csv',\n",
       " 'processed/lapa.csv',\n",
       " \"processed/m'boi_mirim.csv\",\n",
       " 'processed/maua.csv',\n",
       " 'processed/mooca.csv',\n",
       " 'processed/parelheiros.csv',\n",
       " 'processed/penha.csv',\n",
       " 'processed/perus.csv',\n",
       " 'processed/pinheiros.csv',\n",
       " 'processed/pirituba.csv',\n",
       " 'processed/riacho_grande.csv',\n",
       " 'processed/santana_do_parnaiba.csv',\n",
       " 'processed/santo_amaro.csv',\n",
       " 'processed/sao_mateus.csv',\n",
       " 'processed/sao_miguel_paulista.csv',\n",
       " 'processed/se.csv',\n",
       " 'processed/tremembe.csv',\n",
       " 'processed/tucuruvi.csv',\n",
       " 'processed/vila_formosa.csv',\n",
       " 'processed/vila_maria.csv',\n",
       " 'processed/vila_mariana.csv',\n",
       " 'processed/vila_prudente.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_objs = s3_resource.Bucket(\"cge\").objects.filter(Prefix=\"processed\")\n",
    "keys = [obj.key for obj in prefix_objs]\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n",
      "/tmp/ipykernel_1869/3261607407.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[f\"{y}_step_{i+1}\"] = df[y].shift(-i+1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/ml-uhi/notebooks/4.0-modeling_cluster.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baws-server/home/ubuntu/ml-uhi/notebooks/4.0-modeling_cluster.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys[\u001b[39m1\u001b[39m:]:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baws-server/home/ubuntu/ml-uhi/notebooks/4.0-modeling_cluster.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     obj \u001b[39m=\u001b[39m s3_client\u001b[39m.\u001b[39mget_object(Bucket\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcge\u001b[39m\u001b[39m\"\u001b[39m, Key\u001b[39m=\u001b[39mkey)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Baws-server/home/ubuntu/ml-uhi/notebooks/4.0-modeling_cluster.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(io\u001b[39m.\u001b[39;49mBytesIO(obj[\u001b[39m\"\u001b[39;49m\u001b[39mBody\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mread()))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baws-server/home/ubuntu/ml-uhi/notebooks/4.0-modeling_cluster.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Baws-server/home/ubuntu/ml-uhi/notebooks/4.0-modeling_cluster.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdropna(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1049\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/ml-uhi/venv/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[0;32m-> 1433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m   1434\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# getting preprocessed data\n",
    "# prefix_objs = s3_resource.Bucket(\"cge\").objects.filter(Prefix=\"processed\")\n",
    "# keys = [obj.key for obj in prefix_objs]\n",
    "keys = [\n",
    "    'processed/cluster_A.csv',\n",
    "    'processed/cluster_B.csv',\n",
    "    'processed/cluster_C.csv',\n",
    "    'processed/cluster_D.csv'\n",
    "]\n",
    "\n",
    "for key in keys[1:]:\n",
    "    obj = s3_client.get_object(Bucket=\"cge\", Key=key)\n",
    "    df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    df = df.dropna()\n",
    "    \n",
    "    cluster = df.cluster.unique()[0]\n",
    "    ts = df[[\"timestamp\"]]\n",
    "    df = df.drop([\"station\", \"timestamp\", \"cluster\"], axis=1)\n",
    "\n",
    "    # target\n",
    "    y = make_steps(df=df[[\"temperature\"]], y=\"temperature\", steps=6).drop(\"temperature\", axis=1)\n",
    "    y = y.dropna()\n",
    "\n",
    "    # predictors\n",
    "    X = df.drop([\"temperature\"], axis=1)\n",
    "    X = X.loc[y.index.min():y.index.max()]\n",
    "\n",
    "    # scaling data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X_standard = scaler.transform(X)\n",
    "\n",
    "    # train and test data split\n",
    "    test_size = 0.30\n",
    "    X_train_ref, X_test_ref, _, _ = train_test_split(X, y, test_size=0.30, shuffle=False)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_standard, y, test_size=test_size, shuffle=False)\n",
    "\n",
    "    # model training\n",
    "    for model in models:\n",
    "        models[model].fit(X_train, y_train)\n",
    "        y_pred = pd.DataFrame(models[model].predict(X_test), index=X_test_ref.index, columns=y.columns)\n",
    "        ts = ts.loc[y_pred.index.min():y_pred.index.max()]\n",
    "        y_pred[\"timestamp\"] = ts.timestamp \n",
    "        y_pred[\"cluster\"] = cluster\n",
    "\n",
    "        # writing predictions to S3 bucket\n",
    "        cluster_ = unidecode(key.lower().replace(\" \", \"_\").replace(\"processed/\", \"\").replace(\".csv\", \"\"))\n",
    "        file_name = cluster_ + \"_\" + model\n",
    "        buffer = io.StringIO()\n",
    "        y_pred.to_csv(buffer)\n",
    "        s3_resource.Object(\"cge\", f\"output/{file_name}.csv\").put(Body=buffer.getvalue())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
